{"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"498a8222ea08461eae1b036026fca9d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f5e5c8bbf7d4b77a8ef59315ac8a7e5","IPY_MODEL_65cbb1f9b9014a22a97ef8cfe7f54304","IPY_MODEL_de418f93f49a436a94cf43574aec814c"],"layout":"IPY_MODEL_e62dc4dbcfb14d1d9ac74cffb4060750"}},"7f5e5c8bbf7d4b77a8ef59315ac8a7e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17008b3e5fc5414db8c0e848089969fe","placeholder":"​","style":"IPY_MODEL_3b8ec84e6a3e4983baaeeb53c635c980","value":"100%"}},"65cbb1f9b9014a22a97ef8cfe7f54304":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_613c37f447e54e3db0656a9dbc205d18","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc49153eb13d433b861bfadf8835aefa","value":50}},"de418f93f49a436a94cf43574aec814c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be73d3b57319464b909787ffc3576608","placeholder":"​","style":"IPY_MODEL_db78a32714f94c06b9f8f350ea2cb87d","value":" 50/50 [02:29&lt;00:00,  2.96s/it]"}},"e62dc4dbcfb14d1d9ac74cffb4060750":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17008b3e5fc5414db8c0e848089969fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b8ec84e6a3e4983baaeeb53c635c980":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"613c37f447e54e3db0656a9dbc205d18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc49153eb13d433b861bfadf8835aefa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be73d3b57319464b909787ffc3576608":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db78a32714f94c06b9f8f350ea2cb87d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n\n<h3 style=\"text-align: center;\"><b>Домашнее задание. Весна 2021</b></h3>\n\n# Autoencoders\n","metadata":{"id":"LQ7i1HkmYY68","editable":false}},{"cell_type":"markdown","source":"# Часть 1. Vanilla Autoencoder (10 баллов)","metadata":{"id":"Wru2LNFuL2Iq","editable":false}},{"cell_type":"markdown","source":"## 1.1. Подготовка данных (0.5 балла)\n","metadata":{"id":"kr3STtdpYY7G","editable":false}},{"cell_type":"code","source":"import numpy as np\nfrom torch.autograd import Variable\nfrom torchvision import datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data_utils\nimport torch\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"xTNi9JLRYY7I","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n                      images_name = \"lfw-deepfunneled\",\n                      dx=80,dy=80,\n                      dimx=64,dimy=64\n    ):\n\n    #download if not exists\n    if not os.path.exists(images_name):\n        print(\"images not found, donwloading...\")\n        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n        print(\"extracting...\")\n        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n        print(\"done\")\n        assert os.path.exists(images_name)\n\n    if not os.path.exists(attrs_name):\n        print(\"attributes not found, downloading...\")\n        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n        print(\"done\")\n\n    #read attrs\n    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n\n\n    #read photos\n    photo_ids = []\n    for dirpath, dirnames, filenames in os.walk(images_name):\n        for fname in filenames:\n            if fname.endswith(\".jpg\"):\n                fpath = os.path.join(dirpath,fname)\n                photo_id = fname[:-4].replace('_',' ').split()\n                person_id = ' '.join(photo_id[:-1])\n                photo_number = int(photo_id[-1])\n                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n\n    photo_ids = pd.DataFrame(photo_ids)\n    # print(photo_ids)\n    #mass-merge\n    #(photos now have same order as attributes)\n    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n\n    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n\n    # print(df.shape)\n    #image preprocessing\n    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n                                .apply(lambda img: resize(img,[dimx,dimy]))\n\n    all_photos = np.stack(all_photos.values)#.astype('uint8')\n    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n    \n    return all_photos, all_attrs","metadata":{"id":"zvAjov5F2NvE","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\nimport os\nimport pandas as pd\nimport skimage.io\nfrom skimage.transform import resize\ndata, attrs = fetch_dataset()","metadata":{"id":"W3KhlblLYY7P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1daa50bc-ce39-4e9f-cc44-017b4c29015d","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nРазбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:","metadata":{"id":"8MSzXXGoYY7X","editable":false}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_t = torch.Tensor(data)\ntrain_photos, val_photos, train_attrs, val_attrs = train_test_split(data_t, attrs,\n                                                                    train_size=0.9, shuffle=False)\nval_photos = val_photos.permute(0, 3, 1, 2)\ntrain_photos = train_photos.permute(0, 3, 1, 2)\n\ntrain_loader = torch.utils.data.DataLoader(train_photos, batch_size=32)\nval_loader = torch.utils.data.DataLoader(val_photos, batch_size=32)","metadata":{"scrolled":true,"id":"dFc8lTm_YY7Y","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_photos.size()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3so7PtHePoyN","outputId":"36792720-d21d-4334-be4d-0a02bf8df532","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 20))\nfor i in range(5):\n    \n    plt.subplot(5, 2, 2*i+1)\n    plt.imshow(train_photos[i].permute(1, 2, 0))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Fj1aJGPf7RWG","outputId":"5dfc3327-021e-4ce8-a8db-fb90ea288ae5","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Архитектура модели (1.5 балла)\nВ этом разделе мы напишем и обучем обычный автоэнкодер.\n\n\n\n<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">\n\n\n^ напомню, что автоэнкодер выглядит вот так","metadata":{"id":"z9CC-DUhYY7i","editable":false}},{"cell_type":"code","source":"dim_code = 1000 # выберите размер латентного вектора","metadata":{"id":"csrNCYh-YY7j","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.modules.linear import Linear\n\n# define a simple linear VAE -- whaaaat?\nclass Autoencoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n           nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=1),\n           nn.ReLU(),\n           nn.Conv2d(32, 50, kernel_size=5, stride=1, padding=1),\n           nn.ReLU(),\n           nn.Conv2d(50, 50, kernel_size=3, stride=1, padding=1),\n           nn.ReLU(),\n           nn.Conv2d(50, 30, kernel_size=2, stride=1, padding=1),\n           nn.Flatten(),\n           nn.Linear(111630, 2304),\n           nn.Linear(2304, dim_code)\n\n         #  nn.MaxPool2d(2, stride=1)\n           )\n        self.decoder = nn.Sequential(\n           nn.Linear(dim_code, 2304),\n           nn.Linear(2304, 111630),\n           nn.Unflatten(1, (30, 61, 61)),\n           nn.ConvTranspose2d(30, 50, kernel_size=2, stride=1, padding=1),\n           nn.ReLU(),\n           nn.ConvTranspose2d(50, 50, kernel_size=3, stride=1, padding=1),\n           nn.ReLU(),\n           nn.ConvTranspose2d(50, 32, kernel_size=5, stride=1, padding=1),\n           nn.ReLU(),\n         #  nn.Dropout(0.2),\n           nn.ConvTranspose2d(32, 3, kernel_size=5, stride=1, padding=1),\n           )\n        \n\n\n    def forward(self, sample):\n      \n        batch, _, _, _ = sample.size()\n        latent = self.encoder(sample)\n        #print(enc.shape)\n       # x = enc.view(batch, -1)\n        \n      #  enc = self.fc_enc1(x)\n      #  latent = self.fc_enc2(enc)\n       # dec = self.fc_dec1(latent)\n       # dec = self.fc_dec2(dec)\n      #  dec = dec.view(batch, 30, 29, 29)\n      #  print(dec.shape)\n        reconstructed = self.decoder(latent)\n        reconstructed = torch.sigmoid(reconstructed)\n        return reconstructed, latent","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!","metadata":{"id":"Fjr-N8AWee-k","editable":false}},{"cell_type":"code","source":"\n# define a simple linear VAE -- whaaaat?\n#class Autoencoder(torch.nn.Module):\n #   def __init__(self):\n  #      super().__init__()\n   #     self.encoder = nn.Sequential(\n    #       nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=1),\n     #      nn.ReLU(),\n      #     nn.Conv2d(32, 128, kernel_size=4, stride=1, padding=1),\n      #     nn.ReLU(),\n      #     nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n       #    nn.ReLU(),\n      #     nn.Conv2d(64, dim_code, kernel_size=2, stride=1, padding=1),\n           \n         #  nn.MaxPool2d(2, stride=1)\n        #   )\n       # self.decoder = nn.Sequential(\n       #    nn.ConvTranspose2d(dim_code, 64, kernel_size=2, stride=1, padding=1),\n       #    nn.ReLU(),\n       #    nn.ConvTranspose2d(64, 128, kernel_size=3, stride=1, padding=1),\n       #    nn.ReLU(),\n       #    nn.ConvTranspose2d(128, 16, kernel_size=4, stride=1, padding=1),\n       #    nn.ReLU(),\n         #  nn.Dropout(0.2),\n        #   nn.ConvTranspose2d(16, 3, kernel_size=5, stride=1, padding=1),\n        #   )\n\n  #  def forward(self, sample):\n  #      latent = self.encoder(sample)\n  #      reconstructed = self.decoder(latent)\n  #      return reconstructed, latent\n","metadata":{"id":"8SjHNX-rYY7k","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = F.mse_loss\n\nautoencoder = Autoencoder()\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n\nmodel = Autoencoder().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"id":"73lg3bI2YY7m","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Обучение (2 балла)","metadata":{"id":"GpntmZCe5L6i","editable":false}},{"cell_type":"markdown","source":"Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n\nА, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)","metadata":{"id":"Bdxg_3WJYY7o","editable":false}},{"cell_type":"code","source":"from tqdm import tqdm_notebook\nimport random\nn_epochs = 50 \ntrain_losses = []\nval_losses = []\n#fig = plt.figure(figsize=(10,7))\n#ax = fig.add_subplot(1, 1, 1)\nfor epoch in tqdm_notebook(range(n_epochs)):\n    model.train()\n    train_losses_per_epoch = []\n    for i, X_batch in enumerate(train_loader):\n    \n        optimizer.zero_grad()\n        reconstruction, _= model(X_batch.to(device))\n        loss = criterion(reconstruction, X_batch.to(device))\n        loss.backward()\n        optimizer.step()\n        train_losses_per_epoch.append(loss.item())\n    train_losses.append(np.mean(train_losses_per_epoch))\n  \n    model.eval()\n    val_losses_per_epoch = []\n    with torch.no_grad():\n        for X_batch in val_loader:\n            reconstruction, _ = model(X_batch.to(device))\n            #reconstruction = reconstruction.view(-1, 3, 64, 64)\n            loss = criterion(reconstruction, X_batch.to(device))\n            val_losses_per_epoch.append(loss.item())\n        random_image = random.randrange(X_batch.shape[0])  \n    #for i, (gt, res) in enumerate(zip(reconstructed[random_image], X_batch[random_image])):\n        plt.figure(figsize=(8, 20))\n        plt.subplot(5, 2, 1)\n        plt.imshow(reconstruction[random_image].cpu().permute(1, 2, 0))\n        plt.subplot(5, 2, 2)\n        plt.imshow(X_batch[random_image].cpu().permute(1, 2, 0))\n\n    val_losses.append(np.mean(val_losses_per_epoch))\n # ax.clear()\n  \n    fig = plt.figure(figsize=(10,7))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.plot(np.arange(len(train_losses)), train_losses, label = 'train')\n    ax.plot(np.arange(len(val_losses)), val_losses, label = 'val')\n \n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(loc='best')\n    plt.show()\n  \n  #ax.plot(np.arange(len(val_losses)), val_losses)\n # plt.show()","metadata":{"scrolled":true,"id":"3H3DOojrYY7o","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["498a8222ea08461eae1b036026fca9d5","7f5e5c8bbf7d4b77a8ef59315ac8a7e5","65cbb1f9b9014a22a97ef8cfe7f54304","de418f93f49a436a94cf43574aec814c","e62dc4dbcfb14d1d9ac74cffb4060750","17008b3e5fc5414db8c0e848089969fe","3b8ec84e6a3e4983baaeeb53c635c980","613c37f447e54e3db0656a9dbc205d18","cc49153eb13d433b861bfadf8835aefa","be73d3b57319464b909787ffc3576608","db78a32714f94c06b9f8f350ea2cb87d"]},"outputId":"7fa52eb2-ff2c-4b08-b130-f575c7c17881","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstruction.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:","metadata":{"id":"FAztAMA4YY7q","editable":false}},{"cell_type":"code","source":"val_photos[i].cpu().permute(1, 2, 0).shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstructed.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>\nlat = []\nfor i in range(5):\n    with torch.no_grad():\n        photo = torch.unsqueeze(val_photos[i], 0)\n        reconstructed, latent_code = model(photo.to(device))\n    plt.figure(figsize=(8, 20))\n    plt.subplot(5, 2, 1)\n    plt.imshow(val_photos[i].cpu().permute(1, 2, 0))\n    plt.subplot(5, 2, 2)\n    plt.imshow(reconstructed.cpu()[0].permute(1, 2, 0))\n    latent_code = torch.unsqueeze(latent_code, 0)\n    lat.append(latent_code)\n    \n    #lat.append(latent_code.cpu())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XRoXungrVS0X","outputId":"edc94196-454d-4b94-d3d7-2587074b22aa","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not bad, right? ","metadata":{"id":"1OPh9O6UYY7s","editable":false}},{"cell_type":"markdown","source":"## 1.4. Sampling (2 балла)","metadata":{"id":"dFi96giuYY7t","editable":false}},{"cell_type":"markdown","source":"Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n\nДавайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n\n__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать.","metadata":{"id":"AOtUaPNYYY7t","editable":false}},{"cell_type":"code","source":"latent_code.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_code","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = torch.rand(25, dim_code, 1).cuda()\nfor i in range(z.shape[0]):\n    \n    with torch.no_grad():\n        photo = torch.unsqueeze(val_photos[i], 0)\n        reconstructed, latent_code = model(photo.to(device))\n        new = latent_code + z[i]\n        dec = model.fc_dec1(new)\n        print(dec.shape)\n        dec = model.fc_dec2(dec)\n\n        print(model.fc_dec2(dec).shape)\n        dec = dec.view(1, 30, 29, 29)\n        new_image = model.decoder(new)\n        plt.figure(figsize=(8, 20))\n        plt.subplot(5, 2, 1)\n        plt.imshow(new_image.cpu()[0].permute(1, 2, 0))\n        plt.subplot(5, 2, 2)\n        plt.imshow(val_photos[i].cpu().permute(1, 2, 0))\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FhpTuii0WIDe","outputId":"df88369a-7676-4dcd-c713-5dfb1fa77986","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time to make fun! (4 балла)\n\nДавайте научимся пририсовывать людям улыбки =)","metadata":{"id":"Ey8dD9s0YY7w","editable":false}},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">","metadata":{"id":"i1v-8WwuYY7w","editable":false}},{"cell_type":"markdown","source":"План такой:\n\n1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n\nНайти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n\n2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n\n3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n\n4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!","metadata":{"id":"eGE0M2GDYY7x","editable":false}},{"cell_type":"code","source":"i = 0\nindex_smile = []\nfor j in range(data_t.shape[0]):\n    if (attrs.Smiling[j] >= 0.8) & () & (i < 15):\n        index_smile.append(j)\n        i += 1","metadata":{"id":"YEYjbClYnfQJ","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_batch.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_t.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_code_smile = []\nfor i in (index_smile):\n    with torch.no_grad():\n        reconstructed, latent_code = model(data_t[i].to(device).permute(2, 1, 0))\n        latent_code = torch.unsqueeze(latent_code, 0)\n        latent_code_smile.append(latent_code)\n    plt.figure(figsize=(8, 20))\n    plt.subplot(5, 2, 1)\n    plt.imshow(data_t[i])\n\nsmile = torch.cat(latent_code_smile)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"S5TWpNuFdz6j","outputId":"5ab920b0-bbe5-4baa-f648-a33c0d7ee878","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attrs.Frowning","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nindex_sad = []\nfor j in range(data_t.shape[0]):\n    if (attrs.Smiling[j] <= -0.5) & (attrs.Frowning[j] >= 0.8)& (i < 15):\n        index_sad.append(j)\n        i += 1","metadata":{"id":"eDCHcjbyrBHm","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_code_sad = []\nfor i in index_sad:\n    with torch.no_grad():\n        reconstructed, latent_code = model(data_t[i].to(device).permute(2, 1, 0))\n        latent_code = torch.unsqueeze(latent_code, 0)\n        latent_code_sad.append(latent_code)\n    plt.figure(figsize=(8, 20))\n    plt.subplot(5, 2, 1)\n    plt.imshow(data_t[i])\n\nsad = torch.cat(latent_code_sad)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mT3d-1YvsnqW","outputId":"d50160e7-70df-4ca6-bca7-88bd72f76c53","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_smile = torch.mean(smile, axis = 0)\nmean_sad = torch.mean(sad, axis = 0)\n\nmean_smile = torch.unsqueeze(mean_smile, 0)\nmean_sad = torch.unsqueeze(mean_sad, 0)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"m8vqOaN3tm6R","outputId":"da59af0e-9f75-4998-aaa6-c60c77c1053a","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_sad.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in index_sad:\n    with torch.no_grad():\n        reconstructed, latent_code = model(data_t[i].to(device).permute(2, 1, 0))\n        latent_code = torch.unsqueeze(latent_code, 0)\n        new = latent_code + mean_smile - mean_sad\n        new_image = model.decoder(new)\n        plt.figure(figsize=(8, 20))\n        plt.subplot(5, 2, 1)\n        plt.imshow(new_image[0].cpu().permute(2, 1, 0))\n        plt.subplot(5, 2, 2)\n        plt.imshow(data_t[i].cpu())","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вуаля! Вы восхитительны!","metadata":{"id":"bXI6jprOYY7z","editable":false}},{"cell_type":"markdown","source":"Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs`:)","metadata":{"id":"E2UAf0bpYY70","editable":false}},{"cell_type":"markdown","source":"# Часть 2: Variational Autoencoder (10 баллов) ","metadata":{"id":"QQnEGmknYY71","editable":false}},{"cell_type":"markdown","source":"Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9","metadata":{"id":"bWQNRjJq2uTz","editable":false}},{"cell_type":"code","source":"batch_size = 32\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False","metadata":{"id":"qBXXr9njByYC","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Архитектура модели и обучение (2 балла)\n\nРеализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре:) Экспериментируйте!","metadata":{"id":"2rHphW5l8Wgi","editable":false}},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self):\n        <определите архитектуры encoder и decoder\n        помните, у encoder должны быть два \"хвоста\", \n        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n\n    def encode(self, x):\n        <реализуйте forward проход энкодера\n        в качестве ваозвращаемых переменных -- mu и logsigma>\n        \n        return mu, logsigma\n    \n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n        else:\n            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n            # на инференсе выход автоэнкодера должен быть детерминирован.\n            return mu\n    \n    def decode(self, z):\n        <реализуйте forward проход декодера\n        в качестве возвращаемой переменной -- reconstruction>\n        \n        return reconstruction\n\n    def forward(self, x):\n        <используя encode и decode, реализуйте forward проход автоэнкодера\n        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n        return mu, logsigma, reconstruction","metadata":{"id":"IoNVT5tYYY74","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Определим лосс и его компоненты для VAE:","metadata":{"id":"hAB77d-PYY76","editable":false}},{"cell_type":"markdown","source":"Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n\nОбщий лосс будет выглядеть так:\n\n$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n\nФормула для KL-дивергенции:\n\n$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n\nВ качестве log-likelihood возьмем привычную нам кросс-энтропию.","metadata":{"id":"UxJrkXGQo5bp","editable":false}},{"cell_type":"code","source":"def KL_divergence(mu, logsigma):\n    \"\"\"\n    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n    \"\"\"\n    loss = <напишите код для KL-дивергенции, пользуясь формулой выше>\n    return \n\ndef log_likelihood(x, reconstruction):\n    \"\"\"\n    часть функции потерь, которая отвечает за качество реконструкции (как mse в обычном autoencoder)\n    \"\"\"\n    loss = <binary cross-entropy>\n    return loss(reconstruction, x)\n\ndef loss_vae(x, mu, logsigma, reconstruction):\n    return <соедините тут две компоненты лосса. Mind the sign!>","metadata":{"id":"ac5ey7uIYY77","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"И обучим модель:","metadata":{"id":"ZPJQu70eYY79","editable":false}},{"cell_type":"code","source":"criterion = loss_vae\n\nautoencoder = VAE()\n\noptimizer = <Ваш любимый оптимизатор>","metadata":{"id":"dtCjfqXdYY79","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"<обучите модель на датасете MNIST>","metadata":{"scrolled":true,"id":"rY1khca6YY7_","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:","metadata":{"id":"SkxW_8fkYY8B","editable":false}},{"cell_type":"code","source":"< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>","metadata":{"id":"4Jd3BWM_YY8C","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:","metadata":{"id":"PQXYIXjoYY8F","editable":false}},{"cell_type":"code","source":"# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\nz = np.array([np.random.normal(0, 1, 100) for i in range(10)])\noutput = <скормите z декодеру>\n<выведите тут полученные картинки>","metadata":{"id":"bOhhH-osYY8G","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. Latent Representation (2 балла)","metadata":{"id":"Nzt-ENxCr6ul","editable":false}},{"cell_type":"markdown","source":"Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\nВаша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n\nЭто позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве. \n\nПлюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n\nПодсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n\n\nИтак, план:\n1. Получить латентные представления картинок тестового датасета\n2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр.","metadata":{"id":"uIWy670xr-Uv","editable":false}},{"cell_type":"code","source":"<ваш код получения латентных представлений, применения TSNE и визуализации>","metadata":{"id":"Bk94C6mCsx9c","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Что вы думаете о виде латентного представления?","metadata":{"id":"ifxhsvPss5h_","editable":false}},{"cell_type":"markdown","source":"__Congrats v2.0!__","metadata":{"id":"ESPBHrL3YY8H","editable":false}},{"cell_type":"markdown","source":"## 2.3. Conditional VAE (6 баллов)\n","metadata":{"id":"yIYuKFwijN2U","editable":false}},{"cell_type":"markdown","source":"Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \nДавайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \nИ вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n\nХотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n\nИ в этой части задания мы научимся такие обучать.","metadata":{"id":"c5l8Bu1RPjUx","editable":false}},{"cell_type":"markdown","source":"### Архитектура\n\nНа картинке ниже представлена архитектура простого Conditional VAE.\n\nПо сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n\nТо есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе.","metadata":{"id":"0j8zNIwKPY-6","editable":false}},{"cell_type":"markdown","source":"\n![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n\n![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n\n","metadata":{"id":"Y6YloFEAPeM4","editable":false}},{"cell_type":"markdown","source":"На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma","metadata":{"id":"hxg2tDSfRbLF","editable":false}},{"cell_type":"markdown","source":"Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки.","metadata":{"id":"GpFbSXLaPrm1","editable":false}},{"cell_type":"markdown","source":"P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе.","metadata":{"id":"cX0zxklMPwI2","editable":false}},{"cell_type":"code","source":"class CVAE(nn.Module):\n    def __init__(self):\n        <определите архитектуры encoder и decoder\n        помните, у encoder должны быть два \"хвоста\", \n        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n\n    def encode(self, x, class_num):\n        <реализуйте forward проход энкодера\n        в качестве ваозвращаемых переменных -- mu, logsigma и класс картинки>\n        \n        return mu, logsigma, class_num\n    \n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n        else:\n            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n            # на инференсе выход автоэнкодера должен быть детерминирован.\n            return mu\n    \n    def decode(self, z, class_num):\n        <реализуйте forward проход декодера\n        в качестве возвращаемой переменной -- reconstruction>\n        \n        return reconstruction\n\n    def forward(self, x):\n        <используя encode и decode, реализуйте forward проход автоэнкодера\n        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n        return mu, logsigma, reconstruction","metadata":{"id":"ar701cHOkDKS","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sampling\n","metadata":{"id":"VoMw-IFyP5A2","editable":false}},{"cell_type":"markdown","source":"Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\nДля MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7.","metadata":{"id":"qe1zWyZHkLV2","editable":false}},{"cell_type":"code","source":"<тут нужно научиться сэмплировать из декодера цифры определенного класса>","metadata":{"id":"A0SQIhvNP9Dr","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splendid! Вы великолепны!\n","metadata":{"id":"nAWBu8rzQBgQ","editable":false}},{"cell_type":"markdown","source":"### Latent Representations","metadata":{"id":"Rt2S77cm3O1v","editable":false}},{"cell_type":"markdown","source":"Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n\nОпять же, нужно покрасить точки в разные цвета в зависимости от класса.","metadata":{"id":"Nt7x8Ek_rHTE","editable":false}},{"cell_type":"code","source":"<ваш код получения латентных представлений, применения TSNE и визуализации>","metadata":{"id":"LSCYK7sH3KEc","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Что вы думаете насчет этой картинки? Отличается от картинки для VAE?","metadata":{"id":"ET8IELWu3Z2c","editable":false}},{"cell_type":"markdown","source":"","metadata":{"id":"SWkqHjvTCD_8","editable":false}},{"cell_type":"markdown","source":"# BONUS 1: Denoising\n\n## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя.","metadata":{"id":"KN3D_k5W_WZz","editable":false}},{"cell_type":"markdown","source":"У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них эта бонусная часть задания.","metadata":{"id":"12a1jkpkCsIU","editable":false}},{"cell_type":"markdown","source":"Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума. \nТо есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка.","metadata":{"id":"v8EN-8jlCtmd","editable":false}},{"cell_type":"markdown","source":"<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>","metadata":{"id":"j1OJg6jhlaZl","editable":false}},{"cell_type":"markdown","source":"Для этого нужно взять ваш любимый датасет (датасет лиц из первой части этого задания или любой другой) и сделать копию этого датасета с шумом. \n\nВ питоне шум можно добавить так:","metadata":{"id":"ysI0BCuRDbvm","editable":false}},{"cell_type":"code","source":"noise_factor = 0.5\nX_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape) ","metadata":{"id":"X5e746iVDgSm","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"<тут ваш код обучения автоэнкодера на зашумленных картинках. Не забудтье разбить на train/test!>","metadata":{"id":"9fSPkXMtDpd5","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"<тут проверка, как AE убирает щум с тестовых картинок. Надеюсь, все получилось =)>","metadata":{"id":"B03NQ_sKDvg2","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BONUS 2: Image Retrieval\n\n## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя.","metadata":{"id":"-NDiCPYLm2bY","editable":false}},{"cell_type":"markdown","source":"Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!","metadata":{"id":"xao_27WMm7AL","editable":false}},{"cell_type":"markdown","source":"План:\n\n1. Получаем латентные представления всех лиц тренировочного датасета\n2. Обучаем на них LSHForest `(sklearn.neighbors.LSHForest)`, например, с `n_estimators=50`\n3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!","metadata":{"id":"Y__bdS23ndeY","editable":false}},{"cell_type":"markdown","source":"Немного кода вам в помощь: (feel free to delete everything and write your own)","metadata":{"id":"IksC2ucIoND-","editable":false}},{"cell_type":"code","source":"codes = <поучите латентные представления картинок из трейна>","metadata":{"id":"hK0YpLMRoEa0","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обучаем LSHForest\nfrom sklearn.neighbors import LSHForest\nlshf = LSHForest(n_estimators=50).fit(codes)","metadata":{"id":"KisDrgZdoWdt","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_similar(image, n_neighbors=5):\n  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n  # прогоняет векторы через декодер и получает картинки ближайших людей\n\n  code = <получение латентного представления image>\n    \n  (distances,),(idx,) = lshf.kneighbors(code, n_neighbors=n_neighbors)\n\n  return distances, X_train[idx]","metadata":{"id":"Y_S5zPb5obam","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_similar(image):\n\n  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n    \n    distances,neighbors = get_similar(image,n_neighbors=11)\n    \n    plt.figure(figsize=[8,6])\n    plt.subplot(3,4,1)\n    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n    plt.title(\"Original image\")\n    \n    for i in range(11):\n        plt.subplot(3,4,i+2)\n        plt.imshow(neighbors[i].cpu().numpy().transpose([1,2,0]))\n        plt.title(\"Dist=%.3f\"%distances[i])\n    plt.show()","metadata":{"id":"t2kjV5wupLP_","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"<тут выведите самые похожие лица к какому-нибудь лицу из тестовой части датасета>","metadata":{"id":"w3Ja1UNf_oJq","editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}